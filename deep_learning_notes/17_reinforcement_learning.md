# 强化学习

> 代码及阅读注释：[https://github.com/fangkun119/handson-ml/blob/master/16\_reinforcement\_learning.ipynb](https://github.com/fangkun119/handson-ml/blob/master/16_reinforcement_learning.ipynb)


## 策略梯度： 
用分配来解决回报稀疏和延迟的问题:根据回报的总和来评估一个行为，即在每步之后乘以一次折扣率r（通常r取值为0.95或0.99）

* 例子
    * 代理决定在某个位置决定未来的3步连续执行三次右加速，折扣系数r=0.8
    * 未来的3步得到的回报分别是：+20, 0, -50
    * 那么在这个位置代理所做决定的回报值是 200.8 + 0 - 500.82 = -22

* 通常r取值为0.95或0.99
    * r = 0.95时，第13步时回报的折扣为0.5 （即0.95<sup>13</sup> = 0.5)
    * r = 0.99时，第69步时回报的折扣为0.5 （即0.99<sup>69</sup> = 0.5)

## 马尔可夫状态转移矩阵：

用矩阵来表示马尔可夫决策过程<br/>

**马尔可夫决策过程**：
> 状态 --(采取各种行动的概率)--> 行动 --((当前状态,行动)导致各种新状态的概率)--> 下一个状态(以及环境观测回报(即时回报))

**矩阵：**

1. 状态转移(T)矩阵：(当前状态,行动,新状态) -> 转移概率
2. 观测回报(R)矩阵：(当前状态,行动,新状态) -> 观测回报
3. 可选行动列表：状态 -> 可选行动列表

## 时间差分学习(Temporal difference Learning, TD) 

用来让代理在探索MDP的过程中学习状态转移概率（而不需要在初始时给出）<br/>

计算公式： 

V<sub>k+1</sub>(state) = (1 - α) * V<sub>k</sub>(s) + α * (reward + discount\_rate * V<sub>k</sub>(state'))

其中：

* reward是代理（控制游戏手柄的程序）观察到的回报值
* α是学习率超参数，开始时值比较高、随后逐渐减小让模型收敛

TD根据代理观测到的状态值、回报值来更新新的状态预估值；<br/>
对每个状态s，TD简单第持续跟踪代理离开该状态时获得的即时回报均值，加上期望以后得到的回报，但是并没有考虑所采取的action以及那种action回报最高

## Q学习：

在TD的基础上，从“跟踪状态”细化到“跟踪<状态,action>“，即

* 用Q<sub>k</sub>(state, action)替换TD公式中的V<sub>k</sub>(state)
* 用max(Q<sub>k</sub>(s', a')替换公式中的V<sub>k</sub>(state')

计算公式：Q<sub>k+1</sub>(state, action) = (1 - α) Q<sub>k</sub>(state, action) + α *(reward + discount_rate * max(Qk(s', a'))

对每一个<state, action>，算法持续跟踪代理通过行为action离开状态state时的平均回报，同时加上以后的期望回报。由于目标策略会采取最佳行为，因此对下一个状态采用最大的Q预估值

TD和Q学习都不需要先验的状态转移矩阵、观测回报矩阵，只需要知道每个state下容许采取哪些action就够了

**传统的逼近Q学习**：从状态提取的手工特征的线性组合（例如，最接近幽灵的距离及方向等）来估计Q值

**DQN（Deep Q Network）：**<br/>
DeepMind显示使用深度学习网络会工作的更好，尤其对于复杂问题，并且不需要任何特征工程 通过简单的调整，下面的代码可用学习玩大部分的Atari游戏并且玩的相当好、可以达到超人的程度（具有长时间故事清洁的游戏除外）
