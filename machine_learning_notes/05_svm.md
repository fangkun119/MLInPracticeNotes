# SVM

## 1.特点
SVM通过最大间隔来增强模型的泛化能力，对特征缩放非常敏感 （P137）

## 2.软间隔SVM
相比硬间隔SVM，软间隔SVM使用了松弛变量，容许间隔违例发生，在间隔宽度和伪例数量之间找到一个平衡。软间隔SVM通过超参数C来控制这个平衡

> C越小间隔越宽但是间隔违例也会越多<br/>
> 模型过拟合时，降低C来容许更多违例可以对模型进行正则化

## 3. SVM核函数：处理非线性数据集
### (1) 多项式核：
* 通用的方法之一是添加更多特征、比如多项式特征
* 也可以使用多项式核（其实多项式核不仅适用于SVM还适用于各种机器学习算法）

### (2) 添加相似特征：
* 另一个方法是添加相似特征，这些特征通过相似函数计算得到，可以使用高斯(RBF)核，采用高斯径向基函数作为相似函数

	> 高斯RBF函数可以测量每个实例与特定地标之间的相似度，相似度是一个值域为[0,1]的钟形函数（钟顶的1表示距离很远，钟两边的0表示位置与地标一样）<br/>
	> 用高斯RBF函数计算样本X与N个地标的距离，得到的N维向量就是转换后的新特征

* 如何选择地标 一种方法是每个样本都当做地标，缺点是得到的新向量维度过于高（M个样本那么就是M维向量），计算量巨大
* 使用高斯RBF核的软间隔SVM有两个超参数、gamma和C
	> **Gamma**: 控制钟型函数 增加gamma钟型函数会变窄每个地标影响范围变小、样本受更小范围地标的影响，决策边界围绕单个样本绕弯；减小gamma每个地标影响范围增大，决策边界变得平摊、有助于减少过拟合</br>
   > **C**: 控制和平衡分类面间隔，降低C可以容许更多违例减少过拟合，是所有软间隔SVM都会使用的超参数

###(3) 核函数选择：
* 永远先从多项式核开始尝试
* 训练集不大时、可以试试高斯RBF核
* 有多余的时间和算力时，再使用交叉验证和网格搜索来尝试其他的核函数

## 4. sklearn的SVM库
**LinearSVC**：基于算法优化库liblinear的sklearn实现，计算速度快，但不支持核函数，通过容差超参数epsilon来平衡计算效率和模型角度</br>
**SVC**：基于标准的libsvc库的sklearn实现，支持核技巧

## 5. SVR - 基于SVM的回归
SVM的原理是、让样本尽可能位于分类间隔的两边（位于街道两边）；SVR相反、让样本尽可能位于分类间隔的中间（位于街道上）；
街道宽度由超参数epsilon控制，在间隔内添加更多实例不会影响模型的预测（epsilon不敏感性）
SVR也支持核函数

## 6. 硬间隔、软间隔SVM模型训练原理 P145 - P151
常用核函数：线性核函数，多项式核函数，高斯RBF核函数，Sigmoid核函数

## 7. 在线SVM（增量学习）：P151
原理、梯度下降、Hinge损失函数及x!=1的区间段的可导性（但是收敛速度慢）；也支持核函数（大规模非线性问题需要用神经网络）
