# CH01 Introduction and Machine Learning Project


## 1.机器学习算法
监督学习：K临近、线性回归、Logistic回归、SVM、决策树和随机森林、Boosting、神经网络 <br/> 
聚类：K-Means、分层聚类、最大期望算法 <br/> 
可视化和降维：主成分分析、核主成分分析、局部线性嵌入、t-分布随机近邻嵌入（T-SNE) <br/> 
关联规则：Apriori，Eclat <br/> 
半监督学习：部分标记的数据（如：聚类 -> 类别标签）<br/>
强化学习：观察环境、作出选择、执行操作（自行学习）<br/>
在线学习（增量学习）：需要注意学习率，监控不良数据 <br/>

## 2.挑战
数据量不足 <br/>
数据不具有代表性（数据量太小，数据量采偏）<br/>
数据质量差（噪声）<br/>
无关特征（特征选择、提取）<br/> 
训练数据过拟合（需要简化模型、收集更多训练数据、减少数据中的噪声、增加正则化超参数、早期停止法）<br/>
训练数据欠拟合（需要更复杂强大的模型，通过特征工程提供更好的特征集），减少模型的约束（如减少正则化超参数）<br/>

##3.测试和验证
模型训练：训练集（记录训练误差） + 测试集（测试泛化误差）<br/> 
但是测试集仍然不够，训练很多轮之后，模型的泛化误差有可能仅仅在测试集上效果好 <br/>
需要再保留一部分样本用做验证集，用于最后一轮测试 <br/> 

方法：用K-Fold交叉验证来找到最优超参数（K-Fold可以避免浪费样本数据），确定超参数之后，使用该超参数在全量训练集上再训练一轮，最后再在验证集上测试最后一轮

##5.典型项目过程
(1) 业务目标时什么、属于哪一类的机器学习任务 <br/>
(2) 确定模型后，确定损失函数（训练优化目标）时什么 <br/>
> 以线性回归为例：使用2范数（均方根误差）、还是1范数？<br/>
> 范数的指数越高、关注越大的误差值、忽略越小的误差值、对异常值更敏感 <br/>

(3) 了解使用方如何使用项目的输出，避免对任务产生误解 <br/>
(4) 开发环境以及相关类库、数据集 <br/>
(5) 数据查看和清洗 <br/>
> 数据schema和描述 <br/>
> 有缺失值的字段 <br/>
> 统计属性（数量，均值，最小最大值，标准差，25%50%75%百分位数值，直方图查看各区值分布）<br/>
> 注意因为某些因为上限截断而导致数据失真的字段（可能需要移除、或者为他们换一个标签）<br/>
> 数值缩放 <br/>
> 对重尾数据做转化使其接近钟形分布（例如计算对数）<br/>

(6) 测试集生成（训练集测试集拆分）：
> 该保持一致的拆分方法时，要避免重新随机 P53 P54 <br/>
> 保证分层抽样一致性 P53 P54 <br/>

(7) 可视化和探索数据，寻找特征
> 可视化，如地理位置可视化等 <br/>
> 寻找属性之间的相关性：皮尔逊相关系数（但是只能找到线性相关关系）<br/>
> 绘制属性之间相关性散点图 <br/>
> 尝试组合特征，例如：`total_rooms/house_num`, `bed_rooms/total_rooms`, `population/house_hold_num` <br/>

(8) 将上面步骤固化成代码 <br/>
(9) 训练模型、尝试不同的模型评估训练集损失值（与数据经验值做比较，来初步估算效果）、使用交叉验证评估测试集损失值。根据是欠拟合还是过拟合决定下一步怎么做。通过随机森林等模型输出的特征重要程度来查看选择的特征 <br/>
(10) 找出几个候选模型，对参数进行微调，使用GridSearchCV或RandomSearchCV <br/>
(11) 用验证集的数据评估调参后的最佳模型 <br/>
(12) 启动、监控和维护系统 <br/>

## 6. 数据清洗Transformer
(1) 有缺失值的数据：丢弃行、丢弃字段、设默认值（0，平均数、中位数、…）<br/>
(2) LabelEncoder/OneHotEncoder/DummyEncoder/...：<br/> 
> 文本Label使用OneHot编码（不使用Enum直接映射成整数的原因时，Label之间没有距离远近的概念）

(3) 特征缩放：<br/>
> (a)最大-最小缩放、即归一化、使用MinMaxScaler <br/>
> (b)标准化、减去均值除以方差使其分布具有单位方差受异常值影响更小，使用StandadScaler 

(4) 使用Pipeline将各个转化器串起来<br/>

## 7. 数据清洗技巧
(1) 根据字符串编辑距离对拼写错误的单词做模糊替换 <br/>
(2) 有些字段需要从字符串转换成ID <br/>
(3) 有些数据（值域长尾）做对数变换 <br/>
(4)	有些数据需要根据给定的分界值进行分组，或者简单分成若干份 <br/>
(5)	有些数据需要做one-hot编码（不希望原始特征之间产生关联，例如`28*2=56`）<br/>
(6)	用PCA从n个特征中选出影响力最高的`k<n`个特征 <br/>
(7)	用随机森林变种算法Isolation Forest做异常值清洗 <br/>
(8)	可能需要做特征组合、或者多项式特征 <br/>

## 8.样本不均的处理方法，例如A类样本比B类多很多时
(1)	样本充足时，让Majority类别欠采样 <br/>
> 方法1: 随机欠采样 <br/>
> 方法2: Majority类样本分成若干份、每份与Minority类一起训练一个模型，若干个模型组成一个随机森林 <br/>
> 方法3: 基于聚类的A类分割 

(2) 样本不足时，让B类过采样（重采样）
> 用随机插值法来合成B类数据: STMOE (Synthetic Minority Over-Sampling Tech)

(3)	代价敏感学习（Cost Sensitive Learning）
> 降低A类权值、提高B类权值

## 9.连续特征如何划分成离散特征
(1) 基于等分区间段之间信息熵变化情况 
> `(max - min) / step_len` 分成n份，遍历这n份，计算熵值变化，哪个点熵值变化大，哪个点适合用作分割点（缺点：超参数n取值小时精度不够；取值大时计算量太大）

(2)	基于样本间隔区间段之间的信息熵变化情况 
> N个样本，得到N-1个区间，用这N-1个区间的中值作为分割点，最多计算N-1次（其中有些分割点不影响分类效果、没必要计算）

(3)	随机选择K次分割点，计算信息熵，选择变化最大的那次

> 其实该方法使用最多、越是随机、越能对抗样本分布带来的问题

## 10.用Isolation Forest检查连续特征的异常值
Step1：	随机选特征随机选分割点，生成一颗有一定深度的决策树，计算样本x从根到叶子节点的长度`f(x)` <br/>
Step2：	重复step1一共`i`次，训练`i`棵树，计算样本x在`i`棵树中`f(x)`总和`F(x)` <br/>
Step3：	若样本`X`为异常值，它应当在大多数iTree中很快就从根节点到达叶子节点了，即`F(x)`比较小 <br/>
> [https://blog.csdn.net/ye1215172385/article/details/79762317](https://blog.csdn.net/ye1215172385/article/details/79762317) <br/>
> [https://www.jianshu.com/p/5af3c66e0410?utm_campaign=maleskine](https://www.jianshu.com/p/5af3c66e0410?utm_campaign=maleskine)

￼






