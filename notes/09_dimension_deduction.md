# 降维

## 1.概述
维度的诅咒<br/>
基于投影的降维方法<br/>
流行学习<br/>

## 2.PCA: 主成因分析，基于线性投影
**原理**：在训练集特征空间中找到1根轴、该轴对差异性贡献最高；然后找到正交的第2根轴、对剩余差异性贡献最高；然后第3根轴……<br/>
**特点**：主成分不稳定，稍微打乱训练集，可能得到安全不同的主成分<br/>
**计算方法**：使用矩阵奇异值分解、选取前d高的主成分来还原、投影到低维空间<br/>
**代码**：(1)可以手动做SVD分解 (2)也可以使用sklearn的API<br/>
**方差稀释率**：可以看到每根主轴对整个数据集方差的贡献度<br/>
**决定降到多少维**：根据方差贡献度，以及希望保留训练集方差的百分之多少，sklearn可以直接传入方差保留目标<br/>
**PCA压缩**：用PCA降维后再还原<br/>
**内存问题**：载入整个训练集内存占用太大，有两种方法解决 
> 使用IncrementalPCA：训练集被切分成多个小批量、分批载入<br/>
> 使用Numpy的内存文件映射

**随机PCA**：计算速度快，适合于目标维度远小于训练集特征维度时<br/>

## 3. PCA原理
X<sup>T</sup>·U=[F<sub>1</sub>, F<sub>2</sub>, …, F<sub>n</sub>]<sup>T</sup>·[U<sub>1</sub>, U<sub>2</sub>, …, U<sub>m</sub>] = [Z<sub>1</sub>, Z<sub>2</sub>, …, Z<sub>n</sub>] = Z<br/>

1. 新特征的方差越大，说明映射矩阵越适合PCA（为了简化计算、平移Z的向量、使样本到投影面的距离均值为0），然后计算方差如下：<br/>
Var(Z) = SUM((Z<sub>i</sub>-0)<sup>2</sup>)/n  = Z<sup>T</sup>·Z / n = (XU)<sup>T</sup>·(XU) / n
2.	因为U是代表映射空间坐标轴的单位向量，因此U<sub>i</sub>与U<sub>j</sub>两两正交，需要满足约束条件 U<sub>i</sub>·U<sub>j</sub>=1<br/>

根据1，2将问题转化成拉格朗日求极值问题，当令拉格朗日函数的偏导值为0时，可以得到 X<sup>t</sup>·X·U = (-ƛ)U， 
解该方程的到的向量ƛ=[ƛ<sub>1</sub>,ƛ<sub>2</sub>,...,ƛ<sub>n</sub>]中的n个元素的值就是 X<sup>T</sup>·X的特征值，对应的特征向量就是解得的U = [U<sub>1</sub>, U<sub>2</sub>, …, U<sub>n</sub>]共n个向量。同时因为X<sup>T</sup>·X是对称阵，因此一定有解，并且满足U<sub>1</sub>,U<sub>2</sub>,…,U<sub>n</sub>两两正交。 按由大到小对lambda排序，按**总和的90%**来截断，选择用做降维空间的坐标轴。

## 4.kPCA(核PCA)
相当于先对样本使用核函数升维、再使用线性PCA映射到低维空间，用于复杂的非线性投影降维
如何为核PCA选择最优的核函数和超参数ᴦ<br/>

* 方法1：用核PCA降到低维后用Logistic Regression分类，使用网格搜索找到最佳的组合
* 方法2：选择重建误差最低的超参数，如何计算重建误差：
    * 将原始高维样本从高维空间通过核PCA投影到低维空间
    * 用重建原像的方法从低维空间重新映射回高维空间、叫做重建原像
    * 计算原始高维样本，与重建原像之间的均方根误差

## 5.局部线性嵌入(LLE)
是一种流形学习技术，擅长展开弯曲的流形，特别是没有太多噪声时<br/>
**方法**：首先测量每个样本如何与其最邻近的邻居线性相关，然后为训练集寻找一个能最大限度保留这些局部关系的低维表示</br>
**过程**：</br>

1. 为每个样本X<sub>i</sub>寻找最近的k个邻居，将X<sub>i</sub>重建为这k个邻居的线性函数（让重建值与X<sub>i</sub>原值的距离平方最小）
2. 所有样本重建完之后，可以得到一个覆盖整个训练集的权重矩阵W
3. 将训练集映射到低维空间，同时保持每个样本的映射值Z<sub>i</sub>与其最近的k个邻居的映射值的线性关系能保持（Z<sub>i</sub>与由邻居映射点重建的值距离平方和最小）

**缺点**：计算复杂度高，很难用在大数据集

## 6.MDS
多维缩放，保持实例之间的距离的前提下降低维度

## 6.Isomap（等度量映射）
将每个实例与其最近的邻居连接起来，创建裂解徒刑，然后保留实力之家的这个测地距离（两个节点之间最短路径上的节点数），降低维度

## 7.t-SNE（t-分布随机近邻嵌入）
降维的同时，试图让相似的实例彼此靠近，不相似的实例彼此远离。主要用于可视化，尤其是高维空间的实例集群可视化（如MNIST图像的二维可视化）

## 8.线性判别（LDA）
其实是一个分类算法，但是会在训练过程中学习类别之间最有区别的超平面。在SVM分类之前、LDA也是一个不错的降维手段


